{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d280c06c-1dff-4892-8b91-aafa7655bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb97a295-3787-49db-9023-e1247e202c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c1c055-5d8f-4f72-b79b-890fa42d0f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset:\n",
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in dataset:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934630c1-c7bc-4ad9-9805-d6678064b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93220fa1-3e9a-461e-ae8f-f89f7d58dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5d536-55f0-4ab4-8049-e60af1878cbc",
   "metadata": {},
   "source": [
    "## Explanation of Preprocessing Steps \n",
    "1. Checked for missing values: No missing values found.\n",
    "2. Applied StandardScaler to normalize feature values for better model performance.\n",
    "3. Split data into training (80%) and testing (20%) sets to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3991dd-08e8-4a67-af8d-cf7d50626eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.9737\n",
      "Confusion Matrix:\n",
      "[[41  2]\n",
      " [ 1 70]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.9386\n",
      "Confusion Matrix:\n",
      "[[39  4]\n",
      " [ 3 68]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92        43\n",
      "           1       0.94      0.96      0.95        71\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.9649\n",
      "Confusion Matrix:\n",
      "[[40  3]\n",
      " [ 1 70]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "Support Vector Machine Performance:\n",
      "Accuracy: 0.9737\n",
      "Confusion Matrix:\n",
      "[[41  2]\n",
      " [ 1 70]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "k-Nearest Neighbors Performance:\n",
      "Accuracy: 0.9474\n",
      "Confusion Matrix:\n",
      "[[40  3]\n",
      " [ 3 68]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Classification Algorithm Implementation (5 marks)\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "explanations = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Classification Report\": classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21effe7-f0e2-40a7-bd6d-e652dd2c2425",
   "metadata": {},
   "source": [
    "# Algorithm Explanation\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "        - Uses the sigmoid function to predict probabilities for binary classification.\n",
    "        - Suitable for this dataset because it is simple, interpretable, and works well for binary classification.\n",
    "        - Achieved an accuracy of 97.37% with high precision and recall.\n",
    "    \n",
    "**Decision Tree**\n",
    "        \n",
    "        - Splits data based on feature values and creates a tree structure for decision making.\n",
    "        - Suitable as it can handle both numerical and categorical data but may overfit.\n",
    "        - Achieved an accuracy of 93.86%, slightly lower than other models due to possible overfitting.\n",
    "\n",
    "**Random Forest**\n",
    "       \n",
    "        - Uses multiple decision trees to reduce overfitting and improve accuracy.\n",
    "        - Suitable as it provides high accuracy and works well with missing data.\n",
    "        - Achieved an accuracy of 96.49%, showing its robustness over a single decision tree.\n",
    "  \n",
    "**Support Vector Machine**\n",
    "   \n",
    "        - Finds the best hyperplane to separate classes and uses kernel tricks for non-linearity.\n",
    "        - Suitable as it works well in high-dimensional datasets like this.\n",
    "        - Achieved an accuracy of 97.37%, similar to Logistic Regression, making it a strong classifier.\n",
    "  \n",
    "**k-Nearest Neighbors**\n",
    "   \n",
    "        - Classifies based on the majority class of the nearest neighbors.\n",
    "        - Suitable for small datasets but computationally expensive for large ones.\n",
    "        - Achieved an accuracy of 94.74%, making it slightly less effective than Random Forest or SVM.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3187a40-d057-43ed-8ba5-553170bad0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison Table:\n",
      "                        Accuracy\n",
      "Logistic Regression     0.973684\n",
      "Decision Tree           0.938596\n",
      "Random Forest           0.964912\n",
      "Support Vector Machine  0.973684\n",
      "k-Nearest Neighbors     0.947368\n",
      "Best Performing Model: Logistic Regression\n",
      "Worst Performing Model: Decision Tree\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Model Comparison (2 marks)\n",
    "comparison_df = pd.DataFrame({k: v['Accuracy'] for k, v in results.items()}, index=['Accuracy']).T\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(comparison_df)\n",
    "\n",
    "best_model = comparison_df['Accuracy'].idxmax()\n",
    "worst_model = comparison_df['Accuracy'].idxmin()\n",
    "print(f\"Best Performing Model: {best_model}\")\n",
    "print(f\"Worst Performing Model: {worst_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb73047-21de-4f50-9f20-21e8abcf0c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
